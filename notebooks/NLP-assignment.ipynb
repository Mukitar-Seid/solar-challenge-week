{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf2e798",
   "metadata": {},
   "source": [
    "Download NTK Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f05e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1b66a",
   "metadata": {},
   "source": [
    "Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd6efac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aade33bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Force re-download of punkt (even if partially downloaded earlier)\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b83bbb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "face8660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import core NLP and data libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download required NLTK resources (only needed once)\n",
    "nltk.download('punkt')                      # Tokenizer\n",
    "nltk.download('averaged_perceptron_tagger') # POS tagger\n",
    "nltk.download('maxent_ne_chunker')          # Named Entity Chunker\n",
    "nltk.download('words')                      # Word list for NER\n",
    "nltk.download('stopwords')                  # Stopwords\n",
    "\n",
    "# Import tokenizers and stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714b4f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8fdb885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mukit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize failed. Falling back to spaCy.\n",
      "Original Tokens: ['natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence', '!']\n",
      "Filtered Tokens (after preprocessing): ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n"
     ]
    }
   ],
   "source": [
    "# 1. Install required libraries if not already installed\n",
    "# Uncomment and run once if using a new environment\n",
    "# !pip install nltk spacy matplotlib wordcloud scikit-learn pandas\n",
    "\n",
    "# 2. Import all necessary libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 3. Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 4. Import tokenizers and stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "\n",
    "# 5. Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -------------------------------\n",
    "# âœ… TEXT PREPROCESSING STARTS HERE\n",
    "# -------------------------------\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is a fascinating field of Artificial Intelligence!\"\n",
    "\n",
    "# Step 1: Convert text to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Step 2: Tokenize using NLTK\n",
    "try:\n",
    "    tokens = word_tokenize(text)\n",
    "except LookupError:\n",
    "    print(\"word_tokenize failed. Falling back to spaCy.\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "# Step 3: Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "# Show results\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Filtered Tokens (after preprocessing):\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63aad5",
   "metadata": {},
   "source": [
    "2. Tokenization and N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b36ea3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'fascinating'), ('fascinating', 'field'), ('field', 'artificial'), ('artificial', 'intelligence')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Assuming filtered_tokens contains the cleaned tokens from previous step\n",
    "cleaned_tokens = filtered_tokens\n",
    "\n",
    "# Generate bigrams (2-grams)\n",
    "bigrams = list(ngrams(cleaned_tokens, 2))\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5d88e",
   "metadata": {},
   "source": [
    "3. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e888e2",
   "metadata": {},
   "source": [
    "Use spaCy to perform NER on a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f078ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "Hawaii GPE\n",
      "2008 DATE\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "sentence = \"Barack Obama was born in Hawaii and was elected president in 2008.\"\n",
    "\n",
    "# Process the sentence with spaCy's NLP pipeline\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract and print named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d176b",
   "metadata": {},
   "source": [
    "4. Converting Text to Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f930a21",
   "metadata": {},
   "source": [
    "Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b634b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Output:\n",
      " [[0 0 0 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 1 1 0 0 0 1 1 1 1 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 1]]\n",
      "Feature Names: ['ai' 'future' 'is' 'language' 'learning' 'love' 'machine' 'natural' 'of'\n",
      " 'part' 'processing' 'the']\n",
      "\n",
      "TF-IDF Vectorizer Output:\n",
      " [[0.         0.         0.         0.         0.57735027 0.57735027\n",
      "  0.57735027 0.         0.         0.         0.         0.        ]\n",
      " [0.30650422 0.         0.30650422 0.40301621 0.         0.\n",
      "  0.         0.40301621 0.40301621 0.40301621 0.40301621 0.        ]\n",
      " [0.42804604 0.5628291  0.42804604 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5628291 ]]\n",
      "Feature Names: ['ai' 'future' 'is' 'language' 'learning' 'love' 'machine' 'natural' 'of'\n",
      " 'part' 'processing' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Natural language processing is a part of AI.\",\n",
    "    \"AI is the future.\"\n",
    "]\n",
    "\n",
    "# CountVectorizer: Converts text to a matrix of token counts\n",
    "count_vec = CountVectorizer()\n",
    "X_count = count_vec.fit_transform(sentences)\n",
    "print(\"Count Vectorizer Output:\\n\", X_count.toarray())\n",
    "print(\"Feature Names:\", count_vec.get_feature_names_out())\n",
    "\n",
    "# TfidfVectorizer: Converts text to a matrix of TF-IDF features\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vec.fit_transform(sentences)\n",
    "print(\"\\nTF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())\n",
    "print(\"Feature Names:\", tfidf_vec.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d98600",
   "metadata": {},
   "source": [
    "5. Word Embeddings (Advanced)\n",
    "Use spaCy to get word vectors (embeddings) for given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b021d322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      " [-0.72883    0.20718   -0.0033379 -0.0027673 -0.17204    0.023277\n",
      "  0.1297    -0.2112     0.32876    0.67447    0.10047   -0.30559\n",
      "  0.11213    0.22959   -0.32997    0.1389    -0.57289    2.523\n",
      " -0.32921    0.06045    0.23895    0.1091     0.19358   -0.1765\n",
      "  0.11583    0.63204   -0.13644   -0.24354    0.20061   -0.50244\n",
      "  0.40537   -0.38688    0.73784    0.093937  -0.30643    0.045874\n",
      "  0.097915  -0.082114   0.13082   -0.039022   0.088084  -0.27023\n",
      " -0.077658  -0.0045355  0.18986   -0.063083  -0.138      0.40474\n",
      " -0.16199   -0.10953    0.22923   -0.67634   -0.65763   -0.044595\n",
      " -0.12119    0.071167   0.25993   -0.27052   -0.22474   -0.13818\n",
      "  0.20692    0.87604   -0.35257   -0.1498     0.72804    0.68768\n",
      "  0.19993    0.084733  -0.2234     0.11301    0.29895   -0.090119\n",
      "  0.038172  -0.32912    0.014221  -0.36335    0.5898     0.10467\n",
      "  0.16549    0.47199    0.078939  -0.19985    0.84014   -0.2277\n",
      " -0.22907   -0.26243   -0.32598    1.0146    -0.079235  -0.34248\n",
      "  0.032767   0.49757    0.0047373  0.057762   0.19319    0.10756\n",
      "  0.16938    0.42513   -0.22691    0.095343  -0.094303  -0.3849\n",
      " -0.27853   -0.4554     0.2735    -2.1344     0.040352   0.065232\n",
      "  0.23147    0.13766    0.41638    0.1153     0.61593   -0.012896\n",
      "  0.22778    0.13342    0.12902   -0.0054822 -0.55536    0.56218\n",
      " -0.082096  -0.46214    0.21512   -0.09482   -0.16694   -0.94924\n",
      "  0.08068    0.64041    0.0089012  0.043188  -0.030859   0.02754\n",
      "  0.12671   -0.2175     0.0098018  0.14784    0.37847   -0.32479\n",
      " -0.21623    0.14108    0.069705  -0.18381    0.10896   -0.12666\n",
      " -0.063817  -0.44837   -0.079542  -0.54454    0.32602   -0.089619\n",
      " -0.069878   0.010953  -0.16017   -0.13361    0.37901    0.74596\n",
      " -0.01211   -0.4469     0.16831    0.12325   -0.29108   -0.6984\n",
      " -0.33013   -0.20759   -0.070089   0.023017  -0.32895   -0.02034\n",
      "  0.50521   -0.12432    0.26341   -0.063996  -0.46205   -0.39595\n",
      " -0.15154   -0.22158   -0.050627  -0.015164  -0.38784    0.5011\n",
      "  0.19628   -0.31646   -0.48555   -0.49464    0.35255   -0.060035\n",
      "  0.082212   0.084107   0.17729   -0.55179    0.071874  -0.39032\n",
      "  0.40137   -0.2273     0.35788    0.42503   -0.20496    0.58632\n",
      " -0.2015     0.35892    0.045149  -0.20252    0.15502   -0.019122\n",
      " -0.11768   -0.48471    0.35088    0.14332    0.091038   0.28448\n",
      "  0.35166   -0.87305    0.047971   0.43431   -0.34814    0.035735\n",
      " -0.21673    0.062818  -0.40837   -0.21775    0.1597     0.56172\n",
      " -0.11126    0.33851   -0.31825   -0.10671    0.057792  -0.03997\n",
      " -0.15047   -0.11435    0.56779    0.43056   -0.17674   -0.045657\n",
      "  0.04453   -0.11629    0.094277  -0.46008   -0.12373   -0.18918\n",
      "  0.14565   -0.17643    0.45186   -0.011373   0.16214   -0.17391\n",
      " -0.14195    0.14683   -0.055814   0.52315    0.0032239 -0.51676\n",
      " -0.094159   0.21092    0.22586   -0.78028   -0.67057   -0.031255\n",
      "  0.045657   0.033926   0.16709    0.014854  -0.14456   -0.56059\n",
      " -0.26709   -0.17691    0.15472    0.46348    1.4026    -0.24662\n",
      "  0.3447    -0.56387    0.047655  -0.39049   -0.016389   0.52322\n",
      " -0.22908   -0.31134    0.6579    -0.67904    0.40002    0.055284\n",
      "  0.5956     0.031032  -0.19315   -0.65318   -0.28911    0.13658\n",
      " -0.42426   -0.37742   -0.17644   -0.15885    0.48907    1.0195\n",
      "  0.12087   -0.36731    0.0087637  0.10666   -0.12636    0.66767  ]\n",
      "Vector shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the medium English model (has word vectors)\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Get the vector for the word \"machine\"\n",
    "word = nlp(\"machine\")[0]  # First token\n",
    "print(\"Vector for 'machine':\\n\", word.vector)\n",
    "print(\"Vector shape:\", word.vector.shape)  # Should be (300,)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
